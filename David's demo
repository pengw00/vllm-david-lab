### How to run vLLM locally and test bugs for inference optimization?
  - In the project folder `llm-demo/ ` run `python3 -m venv venv`, which generate a venv folder
    llm-demo/
      --vllm/
      --venv/
  - activate the python environment in venv
    `source venv/bin/activate`

  - make the vllm repo as editable environment
     `pip install -e vllm/`
    
