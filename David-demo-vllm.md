### How to run vLLM locally and test bugs for inference optimization?
  - In the project folder `llm-demo/ ` run `python3 -m venv venv`, which generate a venv folder
    llm-demo/
      --vllm/
      --venv/
  - activate the python environment in venv
    `source venv/bin/activate`

  - make the vllm repo as editable environment
     `pip install -e vllm/`


### Run termimal in kaggle environment
- switch to script mode in notebook and into terminal console
- run ``` !git init ``` 
- run ``` !git clone https://github.com/pengw00/vllm-david-lab.git ```
    
